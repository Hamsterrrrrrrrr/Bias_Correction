{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3c29b9-7c2a-4a6a-9a29-2e95cf694375",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "858e238c-3336-4532-84b3-998ec097d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "\n",
    "from mlp import *\n",
    "from unet import *\n",
    "from lr import *\n",
    "from utils import *\n",
    "from bos import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61d2752b-7262-494b-8bc0-f1a8a57113d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Experiment Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(RANDOM_SEED) \n",
    "np.random.seed(RANDOM_SEED) \n",
    "torch.manual_seed(RANDOM_SEED) \n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# If CUDA (GPU acceleration) is available, set seeds for all GPUs\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED) # Sets seed for current PyTorch GPU\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED) # Sets seed for all available GPUs\n",
    "\n",
    "    # Additional settings for CUDA operations\n",
    "    torch.backends.cudnn.deterministic = True  # Makes cuDNN deterministic (reproducible)\n",
    "    torch.backends.cudnn.benchmark = False # Disables cudnn benchmarking, which automatically selects the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a346fbf-3b3e-4e07-b12b-7f62aedd0ae5",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2080397e-6c31-4cf9-add8-277f6ea9c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSISTENT_BUCKET = 'gs://leap-persistent/jingwenlyuu/S2S'\n",
    "\n",
    "# Week 1 data\n",
    "week1_gefs_1989_2010 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week1/GEFS_pra_1989-2010.zarr').rename({'Y': 'lat', 'X': 'lon'}).mean(dim='M').prcp.transpose('S','lat','lon').values\n",
    "week1_gefs_2011_2018 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week1/GEFS_pra_2011-2018.zarr').rename({'Y': 'lat', 'X': 'lon'}).mean(dim='M').prcp.transpose('S','lat','lon').values\n",
    "week1_imd_1989_2010 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week1/IMD_pra_1989-2010.zarr').rename({'Y': 'lat', 'X': 'lon'}).prcp.transpose('T','lat','lon').values\n",
    "week1_imd_2011_2018 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week1/IMD_pra_2011-2018.zarr').rename({'Y': 'lat', 'X': 'lon'}).prcp.transpose('T','lat','lon').values\n",
    "\n",
    "# Week 2 data\n",
    "week2_gefs_1989_2010 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week2/GEFS_pra_1989-2010.zarr').rename({'Y': 'lat', 'X': 'lon'}).mean(dim='M').prcp.transpose('S','lat','lon').values\n",
    "week2_gefs_2011_2018 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week2/GEFS_pra_2011-2018.zarr').rename({'Y': 'lat', 'X': 'lon'}).mean(dim='M').prcp.transpose('S','lat','lon').values\n",
    "week2_imd_1989_2010 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week2/IMD_pra_1989-2010.zarr').rename({'Y': 'lat', 'X': 'lon'}).prcp.transpose('T','lat','lon').values\n",
    "week2_imd_2011_2018 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week2/IMD_pra_2011-2018.zarr').rename({'Y': 'lat', 'X': 'lon'}).prcp.transpose('T','lat','lon').values\n",
    "\n",
    "# Week 3-4 data\n",
    "week34_gefs_1989_2010 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week34/GEFS_pra_1989-2010.zarr').rename({'Y': 'lat', 'X': 'lon'}).mean(dim='M').prcp.transpose('S','lat','lon').values\n",
    "week34_gefs_2011_2018 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week34/GEFS_pra_2011-2018.zarr').rename({'Y': 'lat', 'X': 'lon'}).mean(dim='M').prcp.transpose('S','lat','lon').values\n",
    "week34_imd_1989_2010 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week34/IMD_pra_1989-2010.zarr').rename({'Y': 'lat', 'X': 'lon'}).prcp.transpose('T','lat','lon').values\n",
    "week34_imd_2011_2018 = xr.open_zarr(f'{PERSISTENT_BUCKET}/training_data/week34/IMD_pra_2011-2018.zarr').rename({'Y': 'lat', 'X': 'lon'}).prcp.transpose('T','lat','lon').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d2b0f2-b789-4d4e-8123-4da76c4eac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "N_VAL = 70  # Half of 2011-2018 data\n",
    "\n",
    "# Week 1 splits\n",
    "# Training data (1989-2010)\n",
    "week1_train_input = week1_gefs_1989_2010\n",
    "week1_train_output = week1_imd_1989_2010\n",
    "\n",
    "# Validation data (first half of 2011-2018)\n",
    "week1_val_input = week1_gefs_2011_2018[:N_VAL]\n",
    "week1_val_output = week1_imd_2011_2018[:N_VAL]\n",
    "\n",
    "# Test data (second half of 2011-2018)\n",
    "week1_test_input = week1_gefs_2011_2018[N_VAL:]\n",
    "week1_test_output = week1_imd_2011_2018[N_VAL:]\n",
    "\n",
    "# Combined train_val\n",
    "week1_train_val_input = np.concatenate([week1_train_input, week1_val_input], axis=0)\n",
    "week1_train_val_output = np.concatenate([week1_train_output, week1_val_output], axis=0)\n",
    "\n",
    "# Combined val_test\n",
    "week1_val_test_input = np.concatenate([week1_val_input, week1_test_input], axis=0)\n",
    "week1_val_test_output = np.concatenate([week1_val_output, week1_test_output], axis=0)\n",
    "\n",
    "# Week 2 splits\n",
    "# Training data (1989-2010)\n",
    "week2_train_input = week2_gefs_1989_2010\n",
    "week2_train_output = week2_imd_1989_2010\n",
    "\n",
    "# Validation data (first half of 2011-2018)\n",
    "week2_val_input = week2_gefs_2011_2018[:N_VAL]\n",
    "week2_val_output = week2_imd_2011_2018[:N_VAL]\n",
    "\n",
    "# Test data (second half of 2011-2018)\n",
    "week2_test_input = week2_gefs_2011_2018[N_VAL:]\n",
    "week2_test_output = week2_imd_2011_2018[N_VAL:]\n",
    "\n",
    "# Combined train_val\n",
    "week2_train_val_input = np.concatenate([week2_train_input, week2_val_input], axis=0)\n",
    "week2_train_val_output = np.concatenate([week2_train_output, week2_val_output], axis=0)\n",
    "\n",
    "# Combined val_test\n",
    "week2_val_test_input = np.concatenate([week2_val_input, week2_test_input], axis=0)\n",
    "week2_val_test_output = np.concatenate([week2_val_output, week2_test_output], axis=0)\n",
    "\n",
    "# Week 3-4 splits\n",
    "# Training data (1989-2010)\n",
    "week34_train_input = week34_gefs_1989_2010\n",
    "week34_train_output = week34_imd_1989_2010\n",
    "\n",
    "# Validation data (first half of 2011-2018)\n",
    "week34_val_input = week34_gefs_2011_2018[:N_VAL]\n",
    "week34_val_output = week34_imd_2011_2018[:N_VAL]\n",
    "\n",
    "# Test data (second half of 2011-2018)\n",
    "week34_test_input = week34_gefs_2011_2018[N_VAL:]\n",
    "week34_test_output = week34_imd_2011_2018[N_VAL:]\n",
    "\n",
    "# Combined train_val\n",
    "week34_train_val_input = np.concatenate([week34_train_input, week34_val_input], axis=0)\n",
    "week34_train_val_output = np.concatenate([week34_train_output, week34_val_output], axis=0)\n",
    "\n",
    "# Combined val_test\n",
    "week34_val_test_input = np.concatenate([week34_val_input, week34_test_input], axis=0)\n",
    "week34_val_test_output = np.concatenate([week34_val_output, week34_test_output], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a81de4-bb4d-4681-8e2c-7e92e8639b8a",
   "metadata": {},
   "source": [
    "# Raw Forecasts (Without Bias Correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10db5c19-7007-45c8-8c06-a0332f1f4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_raw_week1 = ((week1_test_input - week1_test_output)**2 ).mean(axis=0)\n",
    "mse_raw_week2 = ((week2_test_input - week2_test_output)**2 ).mean(axis=0)\n",
    "mse_raw_week34 = ((week34_test_input - week34_test_output)**2 ).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64ac293-644c-4bd0-823d-b5c5a22dcfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations for each lead time\n",
    "corr_raw_week1 = calc_spatial_correlation(week1_test_input, week1_test_output)\n",
    "corr_raw_week2 = calc_spatial_correlation(week2_test_input, week2_test_output)\n",
    "corr_raw_week34 = calc_spatial_correlation(week34_test_input, week34_test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff24ac3e-83fb-4074-9a2a-5a3320031acf",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc373dd-e72f-4f3f-8f99-3bf1fa210a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_lr_week1 = perform_linear_regression(week1_train_val_input, week1_train_val_output, week1_test_input, week1_test_output)\n",
    "pre_lr_week2 = perform_linear_regression(week2_train_val_input, week2_train_val_output, week2_test_input, week2_test_output)\n",
    "pre_lr_week34 = perform_linear_regression(week34_train_val_input, week34_train_val_output, week34_test_input, week34_test_output)\n",
    "\n",
    "mse_lr_week1 = ((pre_lr_week1 - week1_test_output)**2 ).mean(axis=0)\n",
    "mse_lr_week2 = ((pre_lr_week2 - week2_test_output)**2 ).mean(axis=0)\n",
    "mse_lr_week34 = ((pre_lr_week34 - week34_test_output)**2 ).mean(axis=0)\n",
    "\n",
    "cc_lr_week1 = calc_spatial_correlation(pre_lr_week1, week1_test_output)\n",
    "cc_lr_week2 = calc_spatial_correlation(pre_lr_week2, week2_test_output)\n",
    "cc_lr_week34 = calc_spatial_correlation(pre_lr_week34, week34_test_output)\n",
    "\n",
    "ss_lr_week1 = 1 - mse_lr_week1/mse_raw_week1\n",
    "ss_lr_week2 = 1 - mse_lr_week2/mse_raw_week2\n",
    "ss_lr_week34 = 1 - mse_lr_week34/mse_raw_week34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4ba27-f5fa-4690-9f2f-bf7dde829536",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411eb5d-5460-4346-ab88-c79a5c666764",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8412a5c-654a-4288-8353-7d3c0735d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "\n",
    "# week 1\n",
    "x_train_cnn_week1 = torch.from_numpy(week1_train_input).float().unsqueeze(1).to(device)\n",
    "y_train_cnn_week1 = torch.from_numpy(week1_train_output).float().unsqueeze(1).to(device)\n",
    "\n",
    "x_val_cnn_week1 = torch.from_numpy(week1_val_input).float().unsqueeze(1).to(device)\n",
    "y_val_cnn_week1 = torch.from_numpy(week1_val_output).float().unsqueeze(1).to(device)\n",
    "\n",
    "x_test_cnn_week1 = torch.from_numpy(week1_test_input).float().unsqueeze(1).to(device)\n",
    "y_test_cnn_week1 = torch.from_numpy(week1_test_output).float().unsqueeze(1).to(device)\n",
    "\n",
    "train_dataset_cnn_week1 = TensorDataset(x_train_cnn_week1, y_train_cnn_week1)\n",
    "val_dataset_cnn_week1 = TensorDataset(x_val_cnn_week1, y_val_cnn_week1)\n",
    "test_dataset_cnn_week1 = TensorDataset(x_test_cnn_week1, y_test_cnn_week1)\n",
    "\n",
    "\n",
    "train_loader_cnn_week1 = DataLoader(train_dataset_cnn_week1, batch_size=batch_size, shuffle=True)\n",
    "val_loader_cnn_week1 = DataLoader(val_dataset_cnn_week1, batch_size=batch_size, shuffle=False)\n",
    "test_loader_cnn_week1 = DataLoader(test_dataset_cnn_week1, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# week 2\n",
    "x_train_cnn_week2 = torch.from_numpy(week2_train_input).float().unsqueeze(1).to(device)\n",
    "y_train_cnn_week2 = torch.from_numpy(week2_train_output).float().unsqueeze(1).to(device)\n",
    "\n",
    "x_val_cnn_week2 = torch.from_numpy(week2_val_input).float().unsqueeze(1).to(device)\n",
    "y_val_cnn_week2 = torch.from_numpy(week2_val_output).float().unsqueeze(1).to(device)\n",
    "\n",
    "x_test_cnn_week2 = torch.from_numpy(week2_test_input).float().unsqueeze(1).to(device)\n",
    "y_test_cnn_week2 = torch.from_numpy(week2_test_output).float().unsqueeze(1).to(device)\n",
    "\n",
    "train_dataset_cnn_week2 = TensorDataset(x_train_cnn_week2, y_train_cnn_week2)\n",
    "val_dataset_cnn_week2 = TensorDataset(x_val_cnn_week2, y_val_cnn_week2)\n",
    "test_dataset_cnn_week2 = TensorDataset(x_test_cnn_week2, y_test_cnn_week2)\n",
    "\n",
    "train_loader_cnn_week2 = DataLoader(train_dataset_cnn_week2, batch_size=batch_size, shuffle=True)\n",
    "val_loader_cnn_week2 = DataLoader(val_dataset_cnn_week2, batch_size=batch_size, shuffle=False)\n",
    "test_loader_cnn_week2 = DataLoader(test_dataset_cnn_week2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# week 34\n",
    "x_train_cnn_week34 = torch.from_numpy(week34_train_input).float().unsqueeze(1).to(device)\n",
    "y_train_cnn_week34 = torch.from_numpy(week34_train_output).float().unsqueeze(1).to(device)\n",
    "\n",
    "x_val_cnn_week34 = torch.from_numpy(week34_val_input).float().unsqueeze(1).to(device)\n",
    "y_val_cnn_week34 = torch.from_numpy(week34_val_output).float().unsqueeze(1).to(device)\n",
    "\n",
    "x_test_cnn_week34 = torch.from_numpy(week34_test_input).float().unsqueeze(1).to(device)\n",
    "y_test_cnn_week34 = torch.from_numpy(week34_test_output).float().unsqueeze(1).to(device)\n",
    "\n",
    "train_dataset_cnn_week34 = TensorDataset(x_train_cnn_week34, y_train_cnn_week34)\n",
    "val_dataset_cnn_week34 = TensorDataset(x_val_cnn_week34, y_val_cnn_week34)\n",
    "test_dataset_cnn_week34 = TensorDataset(x_test_cnn_week34, y_test_cnn_week34)\n",
    "\n",
    "train_loader_cnn_week34 = DataLoader(train_dataset_cnn_week34, batch_size=batch_size, shuffle=True)\n",
    "val_loader_cnn_week34 = DataLoader(val_dataset_cnn_week34, batch_size=batch_size, shuffle=False)\n",
    "test_loader_cnn_week34 = DataLoader(test_dataset_cnn_week34, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034dc353-4708-4f92-b935-7ad0aa91bc83",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25ecb723-c33b-4165-bfdf-fd9c5171e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_random_seed(42)\n",
    "\n",
    "# criterion = NaNMSELoss()\n",
    "\n",
    "# model_cnn = UNet().to(device)\n",
    "# optimizer = torch.optim.Adam(model_cnn.parameters(), lr=0.001)\n",
    "\n",
    "# cnn_time_1 = train_cnn(model_cnn, train_loader_cnn_week1, val_loader_cnn_week1, criterion, optimizer, device,\n",
    "#             save_path ='/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week1/cnn.pth', n_epochs=300, patience=10)\n",
    "\n",
    "# cnn_time_2 = train_cnn(model_cnn, train_loader_cnn_week2, val_loader_cnn_week2, criterion, optimizer, device,\n",
    "#             save_path ='/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week2/cnn.pth', n_epochs=300, patience=10)\n",
    "\n",
    "# cnn_time_3 = train_cnn(model_cnn, train_loader_cnn_week34, val_loader_cnn_week34, criterion, optimizer, device,\n",
    "#             save_path ='/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week34/cnn.pth', n_epochs=300, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ca03b-d3c6-4c82-9be1-e8d4bf00cf5b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2463bc3c-9aa7-4a30-a24c-e167875bbd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/S2S/Meta-NN/unet.py:270: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model parameters from /home/jovyan/S2S/Meta-NN/gefs_checkpoint/week1/cnn.pth\n",
      "Loaded model parameters from /home/jovyan/S2S/Meta-NN/gefs_checkpoint/week2/cnn.pth\n",
      "Loaded model parameters from /home/jovyan/S2S/Meta-NN/gefs_checkpoint/week34/cnn.pth\n"
     ]
    }
   ],
   "source": [
    "pre_cnn_week1 = evaluate_cnn(model_cnn, device, test_loader_cnn_week1, '/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week1/cnn.pth')\n",
    "mse_cnn_week1 = ((pre_cnn_week1 - week1_test_output)**2 ).mean(axis=0)\n",
    "cc_cnn_week1 = calc_spatial_correlation(pre_cnn_week1, week1_test_output)\n",
    "ss_cnn_week1 = 1 - mse_cnn_week1/mse_raw_week1\n",
    "\n",
    "pre_cnn_week2 = evaluate_cnn(model_cnn, device, test_loader_cnn_week2, '/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week2/cnn.pth')\n",
    "mse_cnn_week2 = ((pre_cnn_week2 - week2_test_output)**2 ).mean(axis=0)\n",
    "cc_cnn_week2 = calc_spatial_correlation(pre_cnn_week2, week2_test_output)\n",
    "ss_cnn_week2 = 1 - mse_cnn_week2/mse_raw_week2\n",
    "\n",
    "pre_cnn_week34 = evaluate_cnn(model_cnn, device, test_loader_cnn_week34, '/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week34/cnn.pth')\n",
    "mse_cnn_week34 = ((pre_cnn_week34 - week34_test_output)**2 ).mean(axis=0)\n",
    "cc_cnn_week34 = calc_spatial_correlation(pre_cnn_week34, week34_test_output)\n",
    "ss_cnn_week34 = 1 - mse_cnn_week34/mse_raw_week34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ab56b-c7b8-41f1-b5c2-73b4ce32c31a",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5729301a-14ba-4695-8f0d-761dd4dd5073",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87f6b59d-85ed-45f9-835c-d597c5b3a552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# week 1\n",
    "X_train_ann_week1 = torch.tensor(week1_train_input, dtype=torch.float32)\n",
    "y_train_ann_week1 = torch.tensor(week1_train_output, dtype=torch.float32)\n",
    "X_val_ann_week1 = torch.tensor(week1_val_input, dtype=torch.float32)\n",
    "y_val_ann_week1 = torch.tensor(week1_val_output, dtype=torch.float32)\n",
    "X_test_ann_week1 = torch.tensor(week1_test_input, dtype=torch.float32)\n",
    "y_test_ann_week1 = torch.tensor(week1_test_output, dtype=torch.float32)\n",
    "\n",
    "# week 2\n",
    "X_train_ann_week2 = torch.tensor(week2_train_input, dtype=torch.float32)\n",
    "y_train_ann_week2 = torch.tensor(week2_train_output, dtype=torch.float32)\n",
    "X_val_ann_week2 = torch.tensor(week2_val_input, dtype=torch.float32)\n",
    "y_val_ann_week2 = torch.tensor(week2_val_output, dtype=torch.float32)\n",
    "X_test_ann_week2 = torch.tensor(week2_test_input, dtype=torch.float32)\n",
    "y_test_ann_week2 = torch.tensor(week2_test_output, dtype=torch.float32)\n",
    "\n",
    "# week 34\n",
    "X_train_ann_week34 = torch.tensor(week34_train_input, dtype=torch.float32)\n",
    "y_train_ann_week34 = torch.tensor(week34_train_output, dtype=torch.float32)\n",
    "X_val_ann_week34 = torch.tensor(week34_val_input, dtype=torch.float32)\n",
    "y_val_ann_week34 = torch.tensor(week34_val_output, dtype=torch.float32)\n",
    "X_test_ann_week34 = torch.tensor(week34_test_input, dtype=torch.float32)\n",
    "y_test_ann_week34 = torch.tensor(week34_test_output, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e15221a-e67c-4534-91d4-7105855777f6",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6586f5fa-6092-4dbd-b638-ca607ca9d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_steps, num_rows, num_cols = X_train_ann_week1.shape\n",
    "\n",
    "# set_random_seed(42)\n",
    "\n",
    "# for i in range(num_rows):\n",
    "#     for j in range(num_cols):\n",
    "        \n",
    "#         X_train_loc = X_train_ann_week1[:, i, j].reshape(-1, 1)\n",
    "#         y_train_loc = y_train_ann_week1[:, i, j].reshape(-1, 1)\n",
    "#         X_val_loc = X_val_ann_week1[:, i, j].reshape(-1, 1)\n",
    "#         y_val_loc = y_val_ann_week1[:, i, j].reshape(-1, 1)\n",
    "\n",
    "#         if torch.isnan(X_train_loc).any() or torch.isnan(y_train_loc).any() or \\\n",
    "#            torch.isnan(X_val_loc).any() or torch.isnan(y_val_loc).any():\n",
    "#             # print(f\"Skipping location ({i}, {j}) due to NaN values\")\n",
    "#             continue\n",
    "            \n",
    "#         train_dataset = TensorDataset(X_train_loc, y_train_loc)\n",
    "#         val_dataset = TensorDataset(X_val_loc, y_val_loc)\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "#         model = ANN()\n",
    "\n",
    "#         criterion = nn.MSELoss()\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "#         checkpoint_path = f'/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week1/ann/checkpoint_loc_{i}_{j}.pth'\n",
    "\n",
    "#         ann_time = train_ann(model, train_loader, val_loader, criterion, optimizer, num_epochs=300, checkpoint_path=checkpoint_path, patience=10, scheduler_patience=5)\n",
    "\n",
    "# print(\"Training completed for all locations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370c6d9-895e-4a7d-b14f-bb98ee813ed0",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9aa2762d-8efc-445f-bdb8-abe29293ed7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre_ann_week1 = np.full(X_test_ann_week1.shape, np.nan)\n",
    "pre_ann_week2 = np.full(X_test_ann_week2.shape, np.nan)\n",
    "pre_ann_week34 = np.full(X_test_ann_week34.shape, np.nan)\n",
    "\n",
    "time_steps, num_rows, num_cols = X_train_ann_week1.shape\n",
    "\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "\n",
    "        X_test_loc_week1 = X_test_ann_week1[:, i, j].reshape(-1, 1)\n",
    "        y_test_loc_week1 = y_test_ann_week1[:, i, j].reshape(-1, 1)\n",
    "        \n",
    "        X_test_loc_week2 = X_test_ann_week2[:, i, j].reshape(-1, 1)\n",
    "        y_test_loc_week2 = y_test_ann_week2[:, i, j].reshape(-1, 1)\n",
    "        \n",
    "        X_test_loc_week34 = X_test_ann_week34[:, i, j].reshape(-1, 1)\n",
    "        y_test_loc_week34 = y_test_ann_week34[:, i, j].reshape(-1, 1)\n",
    "\n",
    "        if torch.isnan(y_test_loc_week1).any():\n",
    "            # print(f\"Skipping location ({i}, {j}) due to NaN values in y_test\")\n",
    "            continue\n",
    "\n",
    "        test_dataset_week1 = TensorDataset(X_test_loc_week1, y_test_loc_week1)\n",
    "        test_loader_week1 = DataLoader(test_dataset_week1, batch_size=32)\n",
    "        \n",
    "        test_dataset_week2 = TensorDataset(X_test_loc_week2, y_test_loc_week2)\n",
    "        test_loader_week2 = DataLoader(test_dataset_week2, batch_size=32)\n",
    "        \n",
    "        test_dataset_week34 = TensorDataset(X_test_loc_week34, y_test_loc_week34)\n",
    "        test_loader_week34 = DataLoader(test_dataset_week34, batch_size=32)\n",
    "        \n",
    "        model_week1 = ANN()\n",
    "        model_week2 = ANN()\n",
    "        model_week34 = ANN()\n",
    "\n",
    "        checkpoint_path_week1 = f'/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week1/ann/checkpoint_loc_{i}_{j}.pth'\n",
    "        checkpoint_path_week2 = f'/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week2/ann/checkpoint_loc_{i}_{j}.pth'\n",
    "        checkpoint_path_week34 = f'/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week34/ann/checkpoint_loc_{i}_{j}.pth'\n",
    "\n",
    "        if os.path.exists(checkpoint_path_week1):\n",
    "            model_week1.load_state_dict(torch.load(checkpoint_path_week1))\n",
    "            # print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "        else:\n",
    "            # print(f\"No checkpoint found for location ({i}, {j}), skipping evaluation\")\n",
    "            continue\n",
    "            \n",
    "        if os.path.exists(checkpoint_path_week2):\n",
    "            model_week2.load_state_dict(torch.load(checkpoint_path_week2))\n",
    "            # print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "        else:\n",
    "            # print(f\"No checkpoint found for location ({i}, {j}), skipping evaluation\")\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(checkpoint_path_week34):\n",
    "            model_week34.load_state_dict(torch.load(checkpoint_path_week34))\n",
    "            # print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "        else:\n",
    "            # print(f\"No checkpoint found for location ({i}, {j}), skipping evaluation\")\n",
    "            continue\n",
    "            \n",
    "        predictions_week1 = evaluate_ann(model_week1, test_loader_week1)     \n",
    "        pre_ann_week1[:, i, j] = predictions_week1.flatten()\n",
    "\n",
    "        predictions_week2 = evaluate_ann(model_week2, test_loader_week2)     \n",
    "        pre_ann_week2[:, i, j] = predictions_week2.flatten()\n",
    "\n",
    "        predictions_week34 = evaluate_ann(model_week34, test_loader_week34)     \n",
    "        pre_ann_week34[:, i, j] = predictions_week34.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "574d67e0-ae0c-4425-86a6-f833141c1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ann_week1 = ((pre_ann_week1 - week1_test_output)**2 ).mean(axis=0)\n",
    "cc_ann_week1 = calc_spatial_correlation(pre_ann_week1, week1_test_output)\n",
    "ss_ann_week1 = 1 - mse_ann_week1/mse_raw_week1\n",
    "\n",
    "mse_ann_week2 = ((pre_ann_week2 - week2_test_output)**2 ).mean(axis=0)\n",
    "cc_ann_week2 = calc_spatial_correlation(pre_ann_week2, week2_test_output)\n",
    "ss_ann_week2 = 1 - mse_ann_week2/mse_raw_week2\n",
    "\n",
    "mse_ann_week34 = ((pre_ann_week34 - week34_test_output)**2 ).mean(axis=0)\n",
    "cc_ann_week34 = calc_spatial_correlation(pre_ann_week34, week34_test_output)\n",
    "ss_ann_week34 = 1 - mse_ann_week34/mse_raw_week34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9164a0c-c5d7-4240-9599-50f2d61fae3d",
   "metadata": {},
   "source": [
    "# Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7690c054-5500-4612-bee6-f139af2f8abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/S2S/upload/lr.py:11: RuntimeWarning: invalid value encountered in divide\n",
      "  \n",
      "/home/jovyan/S2S/upload/unet.py:420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model parameters from /home/jovyan/S2S/Meta-NN/gefs_checkpoint/week1/cnn.pth\n",
      "Loaded model parameters from /home/jovyan/S2S/Meta-NN/gefs_checkpoint/week2/cnn.pth\n",
      "Loaded model parameters from /home/jovyan/S2S/Meta-NN/gefs_checkpoint/week34/cnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2247/2900422672.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_week1.load_state_dict(torch.load(checkpoint_path_week1))\n",
      "/tmp/ipykernel_2247/2900422672.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_week2.load_state_dict(torch.load(checkpoint_path_week2))\n",
      "/tmp/ipykernel_2247/2900422672.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_week34.load_state_dict(torch.load(checkpoint_path_week34))\n"
     ]
    }
   ],
   "source": [
    "pre_lr_meta_week1 = perform_linear_regression(week1_train_input, week1_train_output, week1_val_input, week1_val_output)\n",
    "pre_lr_meta_week2 = perform_linear_regression(week2_train_input, week2_train_output, week2_val_input, week2_val_output)\n",
    "pre_lr_meta_week34 = perform_linear_regression(week34_train_input, week34_train_output, week34_val_input, week34_val_output)\n",
    "\n",
    "pre_cnn_meta_week1 = evaluate_cnn(model_cnn, device, val_loader_cnn_week1, '/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week1/cnn.pth')\n",
    "pre_cnn_meta_week2 = evaluate_cnn(model_cnn, device, val_loader_cnn_week2, '/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week2/cnn.pth')\n",
    "pre_cnn_meta_week34 = evaluate_cnn(model_cnn, device, val_loader_cnn_week34, '/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week34/cnn.pth')\n",
    "\n",
    "pre_ann_meta_week1 = np.full(X_val_ann_week1.shape, np.nan)\n",
    "pre_ann_meta_week2 = np.full(X_val_ann_week2.shape, np.nan)\n",
    "pre_ann_meta_week34 = np.full(X_val_ann_week34.shape, np.nan)\n",
    "\n",
    "time_steps, num_rows, num_cols = X_val_ann_week1.shape\n",
    "\n",
    "# Loop through each location\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        # print(f\"Evaluating model for location ({i}, {j})\")\n",
    "        \n",
    "        # Extract data for this location\n",
    "        X_val_loc_week1 = X_val_ann_week1[:, i, j].reshape(-1, 1)\n",
    "        y_val_loc_week1 = y_val_ann_week1[:, i, j].reshape(-1, 1)\n",
    "        \n",
    "        X_val_loc_week2 = X_val_ann_week2[:, i, j].reshape(-1, 1)\n",
    "        y_val_loc_week2 = y_val_ann_week2[:, i, j].reshape(-1, 1)\n",
    "        \n",
    "        X_val_loc_week34 = X_val_ann_week34[:, i, j].reshape(-1, 1)\n",
    "        y_val_loc_week34 = y_val_ann_week34[:, i, j].reshape(-1, 1)\n",
    "\n",
    "        # Check for NaN values in y_val\n",
    "        if torch.isnan(y_val_loc_week1).any():\n",
    "            # print(f\"Skipping location ({i}, {j}) due to NaN values in y_val\")\n",
    "            continue\n",
    "\n",
    "        # Create DataLoader\n",
    "        val_dataset_week1 = TensorDataset(X_val_loc_week1, y_val_loc_week1)\n",
    "        val_loader_week1 = DataLoader(val_dataset_week1, batch_size=32)\n",
    "        \n",
    "        val_dataset_week2 = TensorDataset(X_val_loc_week2, y_val_loc_week2)\n",
    "        val_loader_week2 = DataLoader(val_dataset_week2, batch_size=32)\n",
    "        \n",
    "        val_dataset_week34 = TensorDataset(X_val_loc_week34, y_val_loc_week34)\n",
    "        val_loader_week34 = DataLoader(val_dataset_week34, batch_size=32)\n",
    "        \n",
    "        model_week1 = ANN()\n",
    "        model_week2 = ANN()\n",
    "        model_week34 = ANN()\n",
    "\n",
    "        # Load the checkpoint\n",
    "        checkpoint_path_week1 = f'/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week1/ann/checkpoint_loc_{i}_{j}.pth'\n",
    "        checkpoint_path_week2 = f'/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week2/ann/checkpoint_loc_{i}_{j}.pth'\n",
    "        checkpoint_path_week34 = f'/home/jovyan/S2S/Meta-NN/gefs_checkpoint/week34/ann/checkpoint_loc_{i}_{j}.pth'\n",
    "\n",
    "        if os.path.exists(checkpoint_path_week1):\n",
    "            model_week1.load_state_dict(torch.load(checkpoint_path_week1))\n",
    "            # print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "        else:\n",
    "            # print(f\"No checkpoint found for location ({i}, {j}), skipping evaluation\")\n",
    "            continue\n",
    "            \n",
    "        if os.path.exists(checkpoint_path_week2):\n",
    "            model_week2.load_state_dict(torch.load(checkpoint_path_week2))\n",
    "            # print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "        else:\n",
    "            # print(f\"No checkpoint found for location ({i}, {j}), skipping evaluation\")\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(checkpoint_path_week34):\n",
    "            model_week34.load_state_dict(torch.load(checkpoint_path_week34))\n",
    "            # print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "        else:\n",
    "            # print(f\"No checkpoint found for location ({i}, {j}), skipping evaluation\")\n",
    "            continue\n",
    "            \n",
    "        predictions_meta_week1 = evaluate_ann(model_week1, val_loader_week1)     \n",
    "        pre_ann_meta_week1[:, i, j] = predictions_meta_week1.flatten()\n",
    "\n",
    "        predictions_meta_week2 = evaluate_ann(model_week2, val_loader_week2)     \n",
    "        pre_ann_meta_week2[:, i, j] = predictions_meta_week2.flatten()\n",
    "\n",
    "        predictions_meta_week34 = evaluate_ann(model_week34, val_loader_week34)     \n",
    "        pre_ann_meta_week34[:, i, j] = predictions_meta_week34.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19aa7af6-670f-4f00-ad43-8102858991be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal weights for Week 1:\n",
      "CNN: 0.831\n",
      "ANN: 0.169\n",
      "LR: 0.001\n",
      "Optimization time: 0h 0m 8s\n",
      "\n",
      "Optimal weights for Week 2:\n",
      "CNN: 0.388\n",
      "ANN: 0.609\n",
      "LR: 0.003\n",
      "Optimization time: 0h 0m 8s\n",
      "\n",
      "Optimal weights for Week 3-4:\n",
      "CNN: 0.630\n",
      "ANN: 0.068\n",
      "LR: 0.302\n",
      "Optimization time: 0h 0m 8s\n"
     ]
    }
   ],
   "source": [
    "preds_week1 = {\n",
    "    'CNN': pre_cnn_meta_week1,\n",
    "    'ANN': pre_ann_meta_week1,\n",
    "    'LR': pre_lr_meta_week1  \n",
    "}\n",
    "\n",
    "preds_week2 = {\n",
    "    'CNN': pre_cnn_meta_week2,\n",
    "    'ANN': pre_ann_meta_week2,\n",
    "    'LR': pre_lr_meta_week2\n",
    "}\n",
    "\n",
    "preds_week34 = {\n",
    "    'CNN': pre_cnn_meta_week34,\n",
    "    'ANN': pre_ann_meta_week34,\n",
    "    'LR': pre_lr_meta_week34\n",
    "}\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ensemble_weighting = BayesianEnsembleWeighting(n_models=3)\n",
    "\n",
    "for period, (y_true, preds) in [\n",
    "    (\"Week 1\", (week1_val_output, preds_week1)),\n",
    "    (\"Week 2\", (week2_val_output, preds_week2)),\n",
    "    (\"Week 3-4\", (week34_val_output, preds_week34))\n",
    "]:\n",
    "    weights, opt_time = ensemble_weighting.optimize_weights(y_true, preds)\n",
    "    print(f\"\\nOptimal weights for {period}:\")\n",
    "    for model, weight in weights.items():\n",
    "        print(f\"{model}: {weight:.3f}\")\n",
    "    print(f\"Optimization time: {int(opt_time//3600)}h {int((opt_time%3600)//60)}m {int(opt_time%60)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49634ef8-1276-45f8-bd31-4665778cb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ensemble_week1 = 0.001 * pre_lr_week1 + 0.181 * pre_ann_week1 + 0.818 * pre_cnn_week1\n",
    "pre_ensemble_week2 = 0.009 * pre_lr_week2 + 0.635 * pre_ann_week2 + 0.356 * pre_cnn_week2\n",
    "pre_ensemble_week34 = 0.302 * pre_lr_week34 + 0.067 * pre_ann_week34 + 0.632 * pre_cnn_week34\n",
    "\n",
    "\n",
    "mse_ensemble_week1 = ((pre_ensemble_week1 - week1_test_output)**2 ).mean(axis=0)\n",
    "cc_ensemble_week1 = calc_spatial_correlation(pre_ensemble_week1, week1_test_output)\n",
    "ss_ensemble_week1 = 1 - mse_ensemble_week1/mse_raw_week1\n",
    "\n",
    "mse_ensemble_week2 = ((pre_ensemble_week2 - week2_test_output)**2 ).mean(axis=0)\n",
    "cc_ensemble_week2 = calc_spatial_correlation(pre_ensemble_week2, week2_test_output)\n",
    "ss_ensemble_week2 = 1 - mse_ensemble_week2/mse_raw_week2\n",
    "\n",
    "mse_ensemble_week34 = ((pre_ensemble_week34 - week34_test_output)**2 ).mean(axis=0)\n",
    "cc_ensemble_week34 = calc_spatial_correlation(pre_ensemble_week34, week34_test_output)\n",
    "ss_ensemble_week34 = 1 - mse_ensemble_week34/mse_raw_week34"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
